---
title: "MovieLens Project Report"
author: "William Fang"
date: "29/04/2020"
output:
  pdf_document:
    toc: true
    number_sections: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(include = FALSE, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE, CACHE = TRUE)
```

# Introduction

The goal of this project is to create a movie recommendation system using the MovieLens data set. Using ideas introduced in **PH125.8x: Data Science: Machine Learning**, the aim is to develop a ML algorithm on one subset of data, and use it to predict movie ratings against a validation set.

```{r data-setup, cache = TRUE}
################################
# Create edx set, validation set
################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
 # https://grouplens.org/datasets/movielens/10m/
 # http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
 download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
 colnames(movies) <- c("movieId", "title", "genres")
 movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                            title = as.character(title),
                                            genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
 edx <- movielens[-test_index,]
 temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
      semi_join(edx, by = "movieId") %>%
      semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)
rm(dl, ratings, movies, test_index, temp, movielens, removed)
```
The data set is the 10M version of the MovieLens data set (See @@), with the following structure:

```{r required-packages}
library(tidyverse)
library(knitr)
library(lubridate)
library(caret)
#if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
#library(kableExtra)
```
```{r data-structure, include = TRUE, comment = NA}
#kable(head(edx %>% as_tibble()), caption = "Data Structure")
str(edx, width = 90, strict.width = "cut")
```

Each row is a single rating of a movie by a user.

* Key steps performed

# Analysis

In **PH125.8x: Data Science: Machine Learning**, the following models were explored:

*  $Y_{u,i} = \mu + \varepsilon_{u,i}$ Predict rating of movie $i$ by user $u$ as mean of all ratings. $\epsilon_{u,i}$ represents independent errors sampled from the same distribution centered at 0.
*  $Y_{u,i} = \mu + b_{i} + \varepsilon_{u,i}$ As above, but with a bias added for movie $i$.
*  $Y_{u,i} = \mu + b_{i} + b_{u} + \varepsilon_{u,i}$ As above, but with a bias added for user $u$.
*  $Y_{u,i} = \mu + b_{i}(\lambda) + \varepsilon_{u,i}$ Regularized movie effect model.
*  $Y_{u,i} = \mu + b_{i}(\lambda) + b_{u}(\lambda) + \varepsilon_{u,i}$ Regularized movie and user effect model.

In developing and exploring the prediction models, the `edx` data set is split into training and test sets (`train_set` and `test_set` respectively). The validation set is only used when computing the final accuracy and isn't used as part of development.

@@@@ Check for NAs, do heat map?

```{r create-test-train-sets, CACHE = FALSE}
# Given movie data, split into test and training sets.
split_set <- function(data, test_portion = 0.1) {
  test_index <- createDataPartition(y = data$rating, times = 1, p = test_portion, list = FALSE)
  train_set <- data[-test_index,]
  temp <- data[test_index,]

  # Make sure userId and movieId in test set are also in train set
  test_set <- temp %>% 
    semi_join(train_set, by = "movieId") %>%
    semi_join(train_set, by = "userId")

  # Add rows removed from test set back into train set
  removed <- anti_join(temp, test_set)
  train_set <- rbind(train_set, removed)
  list("test" = test_set, "train" = train_set)
}

# Split following the same method as used for creating edx and validation sets.
set.seed(1, sample.kind="Rounding")
#test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE)
#train_set <- edx[-test_index,]
#temp <- edx[test_index,]

## Make sure userId and movieId in test set are also in train set
#test_set <- temp %>% 
#  semi_join(train_set, by = "movieId") %>%
#  semi_join(train_set, by = "userId")

## Add rows removed from test set back into edx set
#removed <- anti_join(temp, train_set)
#train_set <- rbind(edx, removed)
#rm(test_index, temp, removed)
sets <- split_set(edx)
test_set <- sets$test
train_set <- sets$train
rm(sets)
```

```{r course-models}
# Root mean square error calculation function.
RMSE <- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))
}

# Naive model based on mean of all ratings.
mu <- mean(train_set$rating)
rmse_naive <- RMSE(test_set$rating, mu)
rmse_results <- tibble(method = "Just the average", RMSE = rmse_naive)

# Movie effects model.
movie_avgs <- train_set %>%
  group_by(movieId) %>%
  summarise(b_i = mean(rating - mu))

predicted_ratings <- mu + test_set %>%
  left_join(movie_avgs, by = "movieId") %>%
  pull(b_i)

rmse_movie_effects <- RMSE(test_set$rating, predicted_ratings)
rmse_results <- bind_rows(rmse_results, tibble(method="Movie effects", RMSE=rmse_movie_effects))

# Movie and user effects model.
user_avgs <- train_set %>%
  left_join(movie_avgs, by = "movieId") %>%
  group_by(userId) %>%
  summarise(b_u = mean(rating - mu - b_i))

predicted_ratings <- test_set %>%
  left_join(movie_avgs, by = "movieId") %>%
  left_join(user_avgs, by = "userId") %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)

rmse_movie_user_effects <- RMSE(test_set$rating, predicted_ratings)
rmse_results <- bind_rows(rmse_results, tibble(method="Movie and User effects", RMSE=rmse_movie_user_effects))

# Regularized movie effects model.
# Code is different to course code which used test data for cross validation.

# Create regularized movie model. Returns list of objects that can be used to calculate predictions.
fit_model_reg_movie <- function(train_set, lambda) {
  
  #mu <- mean(train_set$rating)
  #just_the_sum <- train_set %>%
  #  group_by(movieId) %>%
  #  summarise(s = sum(rating - mu), n_i = n())
  #list(mu = mu, just_the_sum = just_the_sum, lambda = lambda)
  
  mu <- mean(train_set$rating)
  b_i <- train_set %>%
    group_by(movieId) %>%
    summarise(b_i = sum(rating - mu) / (n() + lambda))
  list(mu = mu, b_i = b_i)
}

# Predict ratings using model (object obtained from fit_moddel_reg_movie()), and penalty value lambda.
predict_reg_movie <- function(model, test_set) {
  test_set %>%
    #left_join(model$just_the_sum, by='movieId') %>% 
    #mutate(b_i = s / (n_i + model$lambda)) %>%
    #mutate(pred = model$mu + b_i) %>%
    #pull(pred)
    left_join(model$b_i, by = "movieId") %>%
    mutate(pred = model$mu + b_i) %>%
    pull(pred)
}

# Cross validation to determine optimal lambda. Further break up training set into another train
# and test set pair and use these for tuning. K-fold with k = 5

lambdas <- seq(0, 10, 0.25)
K <- 5

# Generate the sets once to save on computation time and reuse for all cross validation situations.
cv_sets <- sapply(1:K, function(k) {
  sets <- split_set(train_set)
  test_set <- sets$test
  train_set <- sets$train
  list(test_set = test_set, train_set = train_set)
})

# Apply K fold cross validation to calculate RMSE values. "func" is a function
# that computes the RMSEs when given a test and train set. This will be called K
# times, with each pair of test and training sets, and final returned RMSEs are
# the average of each result.
calc_kfold_rmses <- function(func) {
  cat("calc_kfold_rmses:")
  k_rmses <- sapply(1:K, function(k) {
    cv_test_set <- cv_sets[, k]$test_set
    cv_train_set <- cv_sets[, k]$train_set
    
    cat(" ", k)
    func(cv_test_set, cv_train_set)
  })
  
  cat("\n")
  rowMeans(k_rmses)
}

# Calculate RMSEs for each lamba using cross validation.
rmses <- calc_kfold_rmses(function(test_set, train_set) {
  sapply(lambdas, function(lambda) {
    model <- fit_model_reg_movie(train_set, lambda)
    pred <- predict_reg_movie(model, test_set)
    RMSE(test_set$rating, pred)
  })
})

# @@@@
# Calculate the RMSEs for each of the lambda values on each cross validation set.
# The final RMSE values are the average (per lambda value).
#k_rmses <- sapply(1:K, function(k) {
#  cv_test_set <- cv_sets[, k]$test_set
#  cv_train_set <- cv_sets[, k]$train_set
#  
#  sapply(lambdas, function(lambda) {
#    model <- fit_model_reg_movie(cv_train_set, lambda)
#    pred <- predict_reg_movie(model, cv_test_set)
#    RMSE(cv_test_set$rating, pred)
#  })
#})
#rmses <- rowMeans(k_rmses)

best_lambda = lambdas[which.min(rmses)]

# Generate model against full training set, carry out predictions against test set, and calc RMSE.
model <- fit_model_reg_movie(train_set, lambda = best_lambda)
predicted_ratings <- predict_reg_movie(model, test_set)

rmse_reg_movie_effects <- RMSE(test_set$rating, predicted_ratings)
rmse_results <- bind_rows(rmse_results, tibble(method="Regularized Movie Effect Model", RMSE=rmse_reg_movie_effects))

# Regularized movie and user effects model.

# Create regularized movie and user model. Returns list of objects that can be used to calculate predictions.
fit_model_reg_movie_user <- function(train_set, lambda = 0) {
  mu <- mean(train_set$rating)
  b_i <- train_set %>%
    group_by(movieId) %>%
    summarise(b_i = sum(rating - mu) / (n() + lambda))
  
  b_u <- train_set %>%
    left_join(b_i, by = "movieId") %>%
    group_by(userId) %>%
    summarise(b_u = sum(rating - b_i - mu) / (n() + lambda))

  list(mu = mu, b_i = b_i, b_u = b_u)
}

# Predict ratings using model (object obtained from fit_moddel_reg_movie()), and penalty value lambda.
predict_reg_movie_user <- function(model, test_set) {
  test_set %>%
    left_join(model$b_i, by = "movieId") %>%
    left_join(model$b_u, by = "userId") %>%
    mutate(pred = model$mu + b_i + b_u) %>%
    pull(pred)
}

# Cross validation to determine optimal lambda.
rmses <- calc_kfold_rmses(function(test_set, train_set) {
  sapply(lambdas, function(lambda) {
    model <- fit_model_reg_movie_user(train_set, lambda)
    pred <- predict_reg_movie_user(model, test_set)
    RMSE(test_set$rating, pred)
  })
})
#@@@@
#k_rmses <- sapply(1:K, function(k) {
#  cv_test_set <- cv_sets[, k]$test_set
#  cv_train_set <- cv_sets[, k]$train_set

#  sapply(lambdas, function(lambda) {
#    model <- fit_model_reg_movie_user(cv_train_set, lambda)
#    pred <- predict_reg_movie_user(model, cv_test_set)
#    RMSE(cv_test_set$rating, pred)
#  })
#})
#rmses <- rowMeans(k_rmses)
best_lambda <- lambdas[which.min(rmses)]
best_lambda_reg_movie_user <- best_lambda

# Generate model against full training set, carry out predictions against test set, and calc RMSE.
model <- fit_model_reg_movie_user(train_set, lambda = best_lambda)
predicted_ratings <- predict_reg_movie_user(model, test_set)

rmse_reg_movie_user_effects <- RMSE(test_set$rating, predicted_ratings)
rmse_results <- bind_rows(rmse_results, tibble(method="Regularized Movie + User Effect Model", RMSE=rmse_reg_movie_user_effects))

# Free up memory.
rm(cv_sets)
```
The results of training and evaluating the models on the train and test sets:
```{r course-results, include = TRUE}
kable(rmse_results, digits=5, caption="RMSE Results")
```

From the data, there are 3 elements that are yet to be incorporated into any of these models, movie title, timestamp of the rating, and movie genres.

## Movie Title

It is not obvious that there would be any predictive power in the title of a movie, distinct from that of the movie effects value $b_i$.

## Timestamp

In the course exercises we were asked to plot the average rating per week. 

```{r avg-rating-per-week-plot, include = TRUE, echo = FALSE}
# Given movie data frame/tibble, add a column called "date" that is the timestamp
# converted to type datetime, rounded to week.
add_week_date <- function(data) {
  data %>%
  mutate(date = as_datetime(timestamp)) %>%
  mutate(date = round_date(date, unit = "week"))
}
  
avg_rating_vs_date <- edx %>%
  add_week_date() %>%
  group_by(date) %>%
  summarize(rating = mean(rating))

avg_rating_vs_date %>%
  ggplot(aes(date, rating)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "loess", span = 0.75, method.args = list(degree = 2))
```
The plot shows that there is slight variation of the average rating over time, so there may be an increase in prediction accuracy if we add another term to the prediction model that accounts for this. The formula is (based on the course exercise) $Y_{u,i} = \mu + b_{i}(\lambda) + b_{u}(\lambda) + f(d_{u,i}) + \varepsilon_{u,i}$ where $d_{u,i}$ is the day that user $u$ rated movie $i$ and $f$ is a smooth function.

The residuals, after removing $mu$ and movie and user effects, look like this:

```{r residuals-avg-rating-vs-week-plot, include = TRUE, echo = FALSE}
model <- fit_model_reg_movie_user(train_set, best_lambda_reg_movie_user)

resids <- train_set %>%
  add_week_date() %>%
  left_join(model$b_i, by = "movieId") %>%
  left_join(model$b_u, by = "userId") %>%
  mutate(resids = rating - model$mu - b_i - b_u) %>%
  group_by(date) %>%
  summarise(resids = mean(resids))
  
resids %>%
  ggplot(aes(date, resids)) +
  geom_point(alpha=0.3) +
  #geom_smooth(method = "loess", span = 0.10, method.args = list(degree = 2))
  geom_smooth()
```

After removal of the movie and user effects, it can be seen that over time there is not that much variation and only small improvement is expected from the $f$ term. The line produced by `geom_smooth()` is using loess with a span value of 0.75, and doesn't fit the data very well. Using a span of 0.1 and degree of 2 produces the following:

```{r residuals-avg-rating-vs-week-plot2, include = TRUE, echo = FALSE}
resids %>%
  ggplot(aes(date, resids)) +
  geom_point(alpha=0.3) +
  geom_smooth(method = "loess", span = 0.10, method.args = list(degree = 2))
```

```{r model-avg-rating-vs-week}

# Regularized movie and user effects model, with time effect component.
fit_model_time_effect <- function(train_set, lambda = best_lambda_reg_movie_user, span = 0.10) {
  model <- fit_model_reg_movie_user(train_set, lambda)
  
  # Calculate residuals i.e. values after removing mu, movie and user effects.
  resids <- train_set %>%
    add_week_date() %>%
    left_join(model$b_i, by = "movieId") %>%
    left_join(model$b_u, by = "userId") %>%
    mutate(resids = rating - model$mu - b_i - b_u) %>%
    group_by(date) %>%
    summarise(resids = mean(resids))
  
  # Fit loess to residuals.
  loess_model <- loess(resids ~ as.numeric(date), degree = 2, span = span,
                       data = resids, control = loess.control(surface = "direct"))
  
  # Residuals aren't needed for predictions, but useful for graphing.
  list(base_model = model, loess_model = loess_model, resids = resids)
}

# Predict ratings using model (object obtained from fit_model_time_effect()).
predict_time_effect <- function(model, test_set) {
  # Add date to test set.
  test_set_with_date <- test_set %>%
    left_join(model$base_model$b_i, by = "movieId") %>%
    left_join(model$base_model$b_u, by = "userId") %>%
    mutate(date = as_datetime(timestamp)) %>%
    mutate(date = round_date(date, unit = "week"))
  
  # Calculate time effect.
  time_effect <- predict(model$loess_model, test_set_with_date$date)
  
  # Compute predictions using all components.
  test_set_with_date %>%
    mutate(pred = model$base_model$mu + b_i + b_u + time_effect) %>%
    pull(pred)
}

model <- fit_model_time_effect(train_set)
predicted_ratings <- predict_time_effect(model, test_set)

rmse_time_effect <- RMSE(test_set$rating, predicted_ratings)
rmse_results <- bind_rows(rmse_results, tibble(method="Regularized Movie + User Effect + Time Effect Model", RMSE=rmse_time_effect))
```

Using loess for $f$, the resulting RMSE value is `r rmse_time_effect`, which is only a slight improvement on the base regularized movie and user effect model with RMSE `r rmse_reg_movie_user_effects`.

### Movie Time Effect
```{r set-sample-size}
sample_size <- 10^5
```
Instead of examining rating against date, an alternative is to explore the relationship between age of a movie and the ratings. Approximate the release of a movie with the date it was first rated, then the age of a movie is the elapsed time between the timestamp of the rating, and the first rating timestamp. To speed up plotting, a subsample of `r sample_size` rows of the `edx` data set is used in the following.
```{r rating-vs-movie-age-plot, include = TRUE}
small <- edx %>% sample_n(10^5)
small <- small %>%
  add_week_date() %>%
  group_by(movieId) %>%
  mutate(movie_age = date - min(date)) %>%
  ungroup()

small %>% ggplot(aes(as.numeric(movie_age, units="weeks"), rating)) +
  geom_point(alpha=0.01) +
  geom_smooth()
```

From the plot we can see that the lower right quadrant is not as densely occupied, indicating that the older a movie is, the less low rating values it receives and there is a slight upward trend in rating. This can be explained by people being willing to watch older movies that are considered good, but there is very little interest in watching older movies that are regarded as bad.

```{r good-rating-vs-movie-age-plot, include = TRUE}
small %>% group_by(movieId) %>%
  filter(mean(rating) >= 2.5) %>%
  ungroup() %>%
  ggplot(aes(as.numeric(movie_age, units="weeks"), rating)) +
  geom_point(alpha=0.01) +
  geom_smooth()
```

```{r bad-rating-vs-movie-age-plot, include = TRUE}
small %>% group_by(movieId) %>%
  filter(mean(rating) < 2.5) %>%
  ungroup() %>%
  ggplot(aes(as.numeric(movie_age, units="weeks"), rating)) +
  geom_point(alpha=0.01) +
  geom_smooth()
```

The above two plots split the data into two sets. One where the average rating of a movie is equal to or greater than 2.5, and the other below 2.5. The plot for the above average movies is similar to the first plot of all movies, while for the below average movies it is different. This indicates that more accurate predictions will likely be obtained by modelling per movie, rather than having a single function applied to all movies. The overall model is then: $Y_{u,i} = \mu + b_{i}(\lambda) + b_{u}(\lambda) + f_{i}(d_{u,i}) + \varepsilon_{u,i}$

The steps involved converting the training data timestamp to `datetime` type, rounded to week resolution. Then computation of the residuals (rating value minus $mu$ and movie and user effects), and loess was fitted on a per movie basis of rating against date. Note that for movies with too few reviews (arbitrarily chosen as 100), no loess model is fitted and those movies will have no movie time effect.

```{r movie-effect-v-time}
# Regularized movie and user effects model, with movie time effect component.
fit_model_movie_time_effect <- function(train_set, lambda = best_lambda_reg_movie_user) {
  model <- fit_model_reg_movie_user(train_set, lambda)
  
  # Calculate residuals i.e. values after removing mu, movie and user effects.
  resids <- train_set %>%
    add_week_date() %>%
    left_join(model$b_i, by = "movieId") %>%
    left_join(model$b_u, by = "userId") %>%
    mutate(resids = rating - model$mu - b_i - b_u)
  
  movie_ids <- unique(resids$movieId)
  # Get the movie IDs that indicate we are at 10%, 20% etc so we can output some progress.
  output_progress_movie_ids <- movie_ids[seq(1, length(movie_ids), length=10)]
  loess_models <- sapply(movie_ids, function(movie_id) {
    if (movie_id %in% output_progress_movie_ids) {
      cat("#" )
    }
    data <- resids %>% filter(movieId == movie_id) %>%
      group_by(date) %>%
      summarise(resids = mean(resids))
    if (nrow(data) > 100) {
       # Only fit if there is a minimum number of points.
       loess(resids ~ as.numeric(date), data=data, control = loess.control(surface = "direct"))
    } else {
       list()
    }
  })
  
  cat("\n")
  loess_models <- tibble(movieId = movie_ids, loess_model = loess_models)

  list(base_model = model, loess_models = loess_models)
}

# Predict ratings using model (object obtained from fit_model_movie_time_effect()).
# Complete data frame used in calculations is returned, with predictions in column "pred"
predict_movie_time_effect_df <- function(model, test_set) {

  test_set_with_date <- test_set %>%
    add_week_date() %>%
    left_join(model$base_model$b_i, by = "movieId") %>%
    left_join(model$base_model$b_u, by = "userId") %>%
    left_join(model$loess_models, by = "movieId")

  # Calculate time effect.
  time_effect <- apply(test_set_with_date, 1, function(x) {
    if (length(x$loess_model) == 0) {
      # No loess model, no bias
      0
    } else {
      predict(x$loess_model, x$date)
    }
  })

  # Compute predictions using all components.
  test_set_with_date %>%
    mutate(time_effect = time_effect) %>%
    mutate(pred = model$base_model$mu + b_i + b_u + time_effect)# %>%
    #pull(pred)
}

# Predict ratings using model (object obtained from fit_model_movie_time_effect()).
predict_movie_time_effect <- function(model, test_set) {
  predict_movie_time_effect_df(model, test_set)$pred
}

model <- fit_model_movie_time_effect(train_set)
predicted_ratings <- predict_movie_time_effect(model, test_set)

rmse_movie_time_effect <- RMSE(test_set$rating, predicted_ratings)
rmse_results <- bind_rows(rmse_results, tibble(method="Regularized Movie + User Effect + Movie Time Effect Model", RMSE=rmse_movie_time_effect))

rm(model)
```

The RMSE calculation obtained after building the model against the training set, and using it to predict against the test set is `r rmse_movie_time_effect`, which is a significant improvement over the base regulated movie and user effects model.

### User Time Effect

In a similar manner to the movie effect varying with time, there may be a time effect applicable to the user effect.

```{r rating-vs-user-age-plot, include = TRUE}
small <- small %>%
  group_by(userId) %>%
  mutate(user_age = date - min(date)) %>%
  ungroup()

small %>% ggplot(aes(as.numeric(user_age, units="weeks"), rating)) +
  geom_point(alpha=0.01) +
  geom_smooth()
```

In the above plot of rating versus user age (age being the time since the user first rated a movie, not the user's actual age), there is a slight dip after the start, before moving back to the average. As per the movie time effect, it is likely to be more accurate to model per user, rather than a trend across all users. The overall model is then: $Y_{u,i} = \mu + b_{i}(\lambda) + b_{u}(\lambda) + f_{i}(d_{u,i}) + f_{u}(d_{u,i}) + \varepsilon_{u,i}$ where $f_{u}(d_{u,i})$ is a time effect per user. Fitting is done in a similar manner to that done for movie time effects.
@@@@ Try the overall one, approximately 70,000 users, not feasible on desktop computer.

```{r user-effect-v-time}
# Regularized movie and user effects model, with movie and user time effect component.
fit_model_movie_user_time_effect <- function(train_set, lambda = best_lambda_reg_movie_user) {
  
  model <- fit_model_movie_time_effect(train_set, lambda)
  
  # Calculate residuals i.e. values after removing mu, movie and user effects, and movie time effect.
  preds <- predict_movie_time_effect_df(model, train_set)
  preds <- preds %>% mutate(resids = train_set$rating - preds)
  
  user_ids <- unique(train_set$userId)
  
  # Get the user IDs that indicate we are at 10%, 20% etc so we can output some progress.
  output_progress_user_ids <- user_ids[seq(1, length(user_ids), length=10)]
  loess_models <- sapply(user_ids, function(user_id) {
    if (user_id %in% output_progress_user_ids) {
      cat("#" )
    }
    data <- preds %>% filter(userId == user_id) %>%
      group_by(date) %>%
      summarise(resids = mean(resids))
    if (nrow(data) > 100) {
       # Only fit if there is a minimum number of points.
       loess(resids ~ as.numeric(date), data=data, control = loess.control(surface = "direct"))
    } else {
       list()
    }
  })
  
  cat("\n")
  loess_models <- tibble(userId = user_ids, loess_model = loess_models)

  list(base_model = model, user_loess_models = loess_models)
}

# Predict ratings using model (object obtained from fit_model_movie_user_time_effect()).
# Complete data frame used in calculations is returned, with predictions in column "pred"
predict_movie_user_time_effect_df <- function(model, test_set) {
  
  # Carry out prediction of base model, then work out user time effect to add.
  base_pred <- predict_movie_time_effect_df(model$base_model, test_set)
  base_pred <- base_pred %>% left_join(model$user_loess_models, by = "userId")
  
  # Calculate time effect.
  time_effect <- apply(base_pred, 1, function(x) {
    if (length(x$user_loess_model) == 0) {
      # No loess model, no bias
      0
    } else {
      predict(x$loess_model, x$date)
    }
  })

  # Add time effect.
  base_pred %>%
    mutate(time_effect = time_effect) %>%
    mutate(pred = pred + time_effect)
}

# Predict ratings using model (object obtained from fit_model_movie_user_time_effect()).
predict_movie_user_time_effect <- function(model, test_set) {
  predict_movie_user_time_effect_df(model, test_set)$pred
}

model <- fit_model_movie_user_time_effect(train_set)
predicted_ratings <- predict_movie_user_time_effect(model, test_set)

rmse_movie_user_time_effect <- RMSE(test_set$rating, predicted_ratings)
rmse_results <- bind_rows(rmse_results, tibble(method="Regularized Movie + User Effect + Movie + User Time Effect Model",
                                               RMSE=rmse_movie_user_time_effect))

#rm(model)

```


# Results

* Average of 3.5 stars, rather than 2.5 could be explained by users viewing and rating movies they expect to like, rather than watching movies they don't expect to enjoy.

* Arbitrary min value of 100 on number of reviews for the movie effect, could be tuned.

* Could use other algorithms for modelling, e.g. knn and examine accuracy.

# Conclusion

# Appendix
## Generation of Training and Validation Sets
```{r data-generation-code}
## Including Plots

##```{r load-kableExtra, include = FALSE}
##if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
##library(kableExtra)
##```
##```{r course-models, echo = FALSE}
##df <- data.frame(
##  Formula=c("$Y_{u,i} = \\mu + \\varepsilon_{u,i}$"),
##  Description=c("Rating by user $u$ for movie $i$ is the mean of all movie ratings."))
###kable(df, format = "latex")
##kable(df)
##rm(df)
##```
##```{r unload-kableExtra, include = FALSE}
### kableExtra hides some dplr functions, so unload it.
##detach("package:kableExtra", unload=TRUE)
##```
##The $\epsilon_{u,1}$ term represents independent errors sampled from the same distribution centered at zero.
```